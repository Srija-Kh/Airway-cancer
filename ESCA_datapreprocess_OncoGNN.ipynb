{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149f8dc2-eee2-4359-b086-a35627d8f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from node2vec import Node2Vec\n",
    "import networkx as nx\n",
    "\n",
    "def Runnode2vec(filepath):\n",
    "    n2v = n2.SparseOTF(p=1, q=0.5, workers=4, verbose=True)\n",
    "    \n",
    "    # Load the edge list with weights\n",
    "    df = pd.read_csv(filepath, sep=\"\\t\")\n",
    "    \n",
    "    # Normalization\n",
    "    df[\"combined_score\"] /= 1000\n",
    "    \n",
    "    df.to_csv(\"weighted_edges.txt\", sep=\" \", header=False, index=False)\n",
    "\n",
    "    # Read the edge list\n",
    "    edge_list = n2v.read_edg(\"weighted_edges.txt\", weighted=True, directed=False)\n",
    "\n",
    "    # Generate embeddings\n",
    "    emd = n2v.embed(dim=128, num_walks=15, walk_length=60, window_size=10, epochs=15)\n",
    "\n",
    "    # Debugging\n",
    "    print(\"Embedding generated. Type:\", type(emd))\n",
    "    print(\"Embedding content:\", emd)\n",
    "    print(\"SparseOTF attributes:\", dir(n2v))\n",
    "\n",
    "    # Handling node IDs\n",
    "    if hasattr(n2v, \"nodes\"):\n",
    "        n2v_emd = pd.DataFrame(emd, index=n2v.nodes)\n",
    "    else:\n",
    "        raise AttributeError(\"SparseOTF object does not have a method or attribute to access node IDs.\")\n",
    "    \n",
    "    # Renaming\n",
    "    n2v_emd.columns = ['network_' + str(col) for col in n2v_emd.columns]\n",
    "\n",
    "    return n2v_emd.reset_index().rename(columns={\"index\": \"ensembl\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2503f462-2cc2-4dfa-9c6f-857060a7178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def ImportDGN():\n",
    "    dgn = pd.read_csv(\"disease_gda_summary.tsv\",sep='\\t')\n",
    "    dgn_dict = pd.read_csv(\"gda_dictionary.csv\", index_col=None)\n",
    "\n",
    "    score_threshold = 0.02\n",
    "    ei_threshold = 0.7\n",
    "\n",
    "    dgn = dgn[['Gene', 'EI_gda', 'Score_gda']]\n",
    "    dgn = dgn.loc[dgn['Score_gda'] >= score_threshold]\n",
    "    dgn = dgn.loc[dgn['EI_gda'] > ei_threshold]\n",
    "    dgn.rename({'Score_gda':'gda_score'}, axis=1, inplace=True)\n",
    "    dgn = dgn.merge(dgn_dict, on=\"Gene\").drop(['Gene'], axis=1)\n",
    "    dgn['gda_score'] = 1\n",
    "\n",
    "    return dgn[['ensembl', 'gda_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2239c3-bb04-4835-88cd-d32dbb318074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def ImportGDC(file_path):\n",
    "    gdc = pd.read_csv(file_path)\n",
    "\n",
    "    feature_name_columns = ['# SSM Affected Cases in Cohort', '# CNV Gain', '# CNV Loss']\n",
    "    \n",
    "    for feature_column in feature_name_columns:\n",
    "        split_columns = gdc[feature_column].replace({',':''}, regex=True).str.split(' ', n=2, expand=True)\n",
    "        gdc[f'{feature_column}_1'] = split_columns[0].astype(float)\n",
    "        gdc[f'{feature_column}_2'] = split_columns[1]\n",
    "        \n",
    "        gdc[f'{feature_column}_3'] = split_columns[2].replace('[^0-9.]', '', regex=True).astype(float)\n",
    "        \n",
    "        gdc[feature_column] = gdc[f'{feature_column}_1'] / gdc[f'{feature_column}_3']\n",
    "        gdc = gdc.drop([f'{feature_column}_1', f'{feature_column}_2', f'{feature_column}_3'], axis=1)\n",
    "\n",
    "    gdc.drop(['Symbol', 'Name', 'Cytoband', 'Type', 'Annotations', 'Survival'], axis=1, inplace=True)\n",
    "\n",
    "    split_columns = gdc['# SSM Affected Cases Across the GDC'].replace({',':''}, regex=True).str.split(' ', n=2, expand=True)\n",
    "    gdc['1'] = split_columns[0].astype(float)\n",
    "    gdc['2'] = split_columns[1]\n",
    "    \n",
    "    gdc['3'] = split_columns[2].replace('[^0-9.]', '', regex=True).astype(float)\n",
    "    \n",
    "    gdc['# SSM Affected Cases Across the GDC'] = gdc['1'] / gdc['3']\n",
    "    gdc = gdc.drop(['1', '2', '3'], axis=1)\n",
    "\n",
    "    gdc = gdc.rename({'# SSM Affected Cases in Cohort': 'nih_ssm_in_cohort',\n",
    "                      '# SSM Affected Cases Across the GDC': 'nih_ssm_across_gdc',\n",
    "                      '# CNV Gain': 'nih_cnv_gain',\n",
    "                      '# CNV Loss': 'nih_cnv_loss',\n",
    "                      'Gene ID': 'ensembl',\n",
    "                      '#Mutations': 'nih_tot_mutations'}, axis=1)\n",
    "\n",
    "    return gdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63532c06-cff9-4a2d-9aa2-fc063d60a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def ImportHPA():\n",
    "    hpa = pd.read_csv(r\"hpa_gene_features.tsv\", sep='\\t').drop_duplicates(subset='Gene')\n",
    "\n",
    "    identifiers = [\n",
    "        \"Gene\",\n",
    "        \"Ensembl\"\n",
    "    ]\n",
    "    discrete_features = [\n",
    "        \"Protein class\",\n",
    "        \"Biological process\",\n",
    "        \"Molecular function\",\n",
    "        \"Disease involvement\",\n",
    "        \"Subcellular location\",\n",
    "    ]\n",
    "    continuous_features = [\n",
    "        \"Tissue RNA - esophagus [NX]\",\n",
    "        \"Single Cell Type RNA - Mucus-secreting cells [NX]\",\n",
    "        \"Single Cell Type RNA - Basal glandular cells [NX]\",\n",
    "        \"Single Cell Type RNA - Undifferentiated cells [NX]\"\n",
    "    ]\n",
    "\n",
    "    hpa_features = hpa.loc[:, hpa.columns.isin(identifiers + discrete_features + continuous_features)]\n",
    "\n",
    "    # Normalization of continuous features\n",
    "    for col in continuous_features:\n",
    "        hpa_features[col] = (hpa_features[col] - hpa_features[col].mean()) / hpa_features[col].std()\n",
    "\n",
    "    def explode(feature):\n",
    "        return feature.apply(lambda x: x.replace(' ', '').split(','))\n",
    "\n",
    "    hpa_clean = hpa.fillna('')\n",
    "    for ft in discrete_features:\n",
    "        hpa_clean[ft] = explode(hpa_clean[ft])\n",
    "\n",
    "    protein_class = hpa_clean[\"Protein class\"].explode().unique()\n",
    "    biological_process = hpa_clean[\"Biological process\"].explode().unique()\n",
    "    molecular_function = hpa_clean[\"Molecular function\"].explode().unique()\n",
    "    disease_involvement = hpa_clean[\"Disease involvement\"].explode().unique()\n",
    "    subcellular_location = hpa_clean[\"Subcellular location\"].explode().unique()\n",
    "    GO_features = np.concatenate([protein_class, biological_process, molecular_function, disease_involvement, subcellular_location])\n",
    "\n",
    "    RowFeatures = pd.DataFrame(data=0, index=hpa_clean['Ensembl'], columns=GO_features)\n",
    "    counter = 0\n",
    "\n",
    "    for index, row in RowFeatures.iterrows():\n",
    "        features = hpa_clean.iloc[counter][['Protein class', 'Biological process', 'Molecular function', 'Disease involvement', 'Subcellular location']].to_list()\n",
    "        flattened = [item for sublist in features for item in sublist if item]\n",
    "        for t in flattened:\n",
    "            row[t] = 1\n",
    "        counter += 1\n",
    "\n",
    "    # Truncated SVD\n",
    "    n_comp = 200\n",
    "    svd = TruncatedSVD(n_components=n_comp)\n",
    "    svdModel = svd.fit(RowFeatures)\n",
    "    visits_emb = svdModel.transform(RowFeatures)\n",
    "    hpa_reduced = pd.DataFrame(data=visits_emb, index=RowFeatures.index).reset_index(names=\"Ensembl\")\n",
    "    \n",
    "    # Merging\n",
    "    continuous_data = hpa_features[['Ensembl'] + continuous_features].drop_duplicates()\n",
    "    hpa_final = pd.merge(hpa_reduced, continuous_data, on='Ensembl', how='left')\n",
    "\n",
    "    # Rename columns\n",
    "    hpa_final.columns = ['hpa_' + str(col) for col in hpa_final.columns]\n",
    "    hpa_final = hpa_final.rename({\n",
    "        'hpa_Ensembl': 'ensembl',\n",
    "        'hpa_Tissue RNA - esophagus [NX]': 'nx_tissue_rna_esophagus',\n",
    "        'hpa_Single Cell Type RNA - Mucus-secreting cells [NX]': 'nx_single_cell_type_mucus_secreting_cells',\n",
    "        'hpa_Single Cell Type RNA - Basal glandular cells [NX]': 'nx_single_cell_type_basal_glandular_cells',\n",
    "        'hpa_Single Cell Type RNA - Undifferentiated cells [NX]': 'nx_single_cell_type_undifferentiated_cells'\n",
    "    }, axis=1)\n",
    "\n",
    "    return hpa_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70da7522-7d1c-4b6d-bf2c-2195d7d8b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_redun(el, verbose=False):\n",
    "    if verbose:\n",
    "        print(\"Original Size: \", len(el))\n",
    "\n",
    "    el_new = el.iloc[:, 0:2].apply(sorted, axis=1)\n",
    "\n",
    "    el_new = pd.DataFrame.from_dict(dict(zip(el_new.index, el_new.values))).T \n",
    "\n",
    "    el_new = el_new.drop_duplicates()\n",
    "    if verbose:\n",
    "        postDrop = len(el_new)\n",
    "        print(\"After Dropping Duplicates: \", len(el_new), \"(-\", len(el)-postDrop, \")\")\n",
    "\n",
    "    el_new = el_new.merge(el, left_on=[el_new.columns[0],el_new.columns[1]],  right_on=[el.columns[0], el.columns[1]])\n",
    "    if verbose:\n",
    "        print(\"After Merging: \", len(el_new), \"(-\", postDrop-len(el_new), \")\")\n",
    "        print()\n",
    "\n",
    "    return el_new.iloc[:, 2:]\n",
    "\n",
    "def map_IDs(el, gmap, verbose = False, dropNaNvalues = True):\n",
    "    gp_map_f = gmap.set_index('#string_protein_id')['alias']\n",
    "\n",
    "    el_converted = el.reset_index(drop=True)\n",
    "\n",
    "    el_converted[el_converted.columns[0]] = el_converted[el_converted.columns[0]].map(gp_map_f)\n",
    "    el_converted[el_converted.columns[1]] = el_converted[el_converted.columns[1]].map(gp_map_f)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"NaN values per Column:\", el_converted[el_converted.columns[0]].isna().sum(), el_converted[el_converted.columns[1]].isna().sum())\n",
    "\n",
    "    if dropNaNvalues:\n",
    "        el_converted = el_converted.dropna()\n",
    "        if verbose:\n",
    "            print(\"New edge list size:\", len(el_converted), \"( -\", len(el)-len(el_converted), \")\")\n",
    "    \n",
    "    return el_converted\n",
    "\n",
    "def ImportSTRING():\n",
    "    el_map = pd.read_csv(r\"9606.protein.aliases.v12.0.txt\", sep=\"\\t\")\n",
    "    el = pd.read_csv(r\"9606.protein.links.v12.0_sc.txt\", sep=\" \")\n",
    "    el_map = el_map.loc[el_map.source == 'Ensembl_gene']\n",
    "\n",
    "    el = remove_redun(el, True)\n",
    "    el = map_IDs(el, el_map, verbose=True)\n",
    "\n",
    "    return el\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc394e5d-e475-46b7-9878-a26b93c9e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdc = ImportGDC('gdc_esca_fin.csv')\n",
    "gdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ccc25a-cb29-41eb-b2dd-215c17dd7ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpa = ImportHPA()\n",
    "hpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6947e999-2b07-41d0-9d69-0c752bb94581",
   "metadata": {},
   "outputs": [],
   "source": [
    "el = ImportSTRING()\n",
    "el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5064bc5-8bb9-4995-a5b7-7520485d1db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = hpa.merge(gdc, on=\"ensembl\")\n",
    "master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb5e3b5-38f3-4ea5-81ea-6787689b179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "el_allgenes = pd.concat([el['protein1'], el['protein2']]).drop_duplicates()\n",
    "master = master.loc[master['ensembl'].isin(el_allgenes)]\n",
    "master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a3c7c-def7-4b23-8595-a20f7c3ebeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "el_intersect = (\n",
    "    el.iloc[:, :3] #Weights\n",
    "    .merge(master[\"ensembl\"], right_on=\"ensembl\", left_on='protein1')\n",
    "    .drop(\"ensembl\", axis=1)\n",
    ")\n",
    "el_intersect = (\n",
    "    el_intersect\n",
    "    .merge(master[\"ensembl\"], right_on=\"ensembl\", left_on='protein2')\n",
    "    .drop(\"ensembl\", axis=1)\n",
    "    .rename(columns={'protein1': 'gene1', 'protein2': 'gene2'})\n",
    ")\n",
    "\n",
    "el = el_intersect.merge(el, right_on=['protein1', 'protein2'], left_on=['gene1', 'gene2']).drop(['protein1', 'protein2'], axis=1)\n",
    "el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec238b-10ea-4250-b454-7524617fbedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "el[['gene1', 'gene2']].to_csv('eso_edge_list_latest.edg', index=False, header=False, sep='\\t') \n",
    "el.to_csv('esca_features_latest.edg', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c68262-86f9-4924-b93f-a3395d8a2f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "el['gene1'] = pd.to_numeric(el['gene1'], errors='coerce')\n",
    "el['gene2'] = pd.to_numeric(el['gene2'], errors='coerce')\n",
    "el['combined_score_x'] = pd.to_numeric(el['combined_score_x'], errors='coerce')\n",
    "\n",
    "el.dropna(inplace=True)\n",
    "\n",
    "genes_array = el[['gene1', 'gene2', 'combined_score_x']].to_numpy(dtype=np.int64)\n",
    "\n",
    "np.save('eso_edge_list_latest.npy', genes_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab12236-ab49-4d49-9c0e-3d6be8ee1064",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgn = ImportDGN()\n",
    "dgn = dgn.loc[dgn['ensembl'].isin(master['ensembl'])]\n",
    "dgn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9b0536-efac-48eb-ba13-7bae522a4bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Runnode2vec(\"eso_edge_list_latest.edg\")\n",
    "master = master.merge(network, on='ensembl')\n",
    "master.to_csv(\"node_node2vec_data_latest.csv\", index=None)\n",
    "master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cb0b28-37fa-4014-9e6f-9bbd0a39759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "master[\"gda_score\"] = np.nan  \n",
    "\n",
    "#\"label\" to 1 for genes present in 'dgn'\n",
    "master.loc[master[\"ensembl\"].isin(dgn[\"ensembl\"]), \"gda_score\"] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904fde2d-f433-4898-9c3c-e921af7d9d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ones = (master.iloc[:, -1] == 1).sum()\n",
    "print(\"No. of 1s in the last column:\", num_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d074f1-019e-4907-8122-bd809f6d5385",
   "metadata": {},
   "outputs": [],
   "source": [
    "master.to_csv(\"200_node_network_embeddings_latest.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4166ca38-20a3-4886-a12e-d499980ec147",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
