{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4e8bb2-5300-490f-944c-67f2a50741ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(314159)\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fdf6f8-227c-4239-98a5-3bee03a62a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "edge_list = np.load(\"edge_list_latest1.npy\", allow_pickle=True)\n",
    "\n",
    "# Splitting\n",
    "edge_list = np.array([row[0].split(\",\") for row in edge_list[1:]])  # Skip the header\n",
    "\n",
    "# Source nodes, target nodes, and combined scores\n",
    "protein1 = edge_list[:, 0]  # First column (gene1)\n",
    "protein2 = edge_list[:, 1]  # Second column (gene2)\n",
    "combined_score = edge_list[:, 2].astype(np.float32)  # Third column (combined_score_x or _y)\n",
    "\n",
    "# Mapping\n",
    "protein_map = {protein: idx for idx, protein in enumerate(np.unique(np.concatenate((protein1, protein2))))}\n",
    "protein1 = np.vectorize(protein_map.get)(protein1)\n",
    "protein2 = np.vectorize(protein_map.get)(protein2)\n",
    "\n",
    "# Stacking\n",
    "edges = np.stack([protein1, protein2], axis=0)\n",
    "edge_list = torch.tensor(edges, dtype=torch.long)\n",
    "edge_weight = torch.tensor(combined_score, dtype=torch.float32) \n",
    "\n",
    "print(\"Edge list shape:\", edge_list.shape) \n",
    "print(\"Edge weights shape:\", edge_weight.shape)\n",
    "print(\"Protein map:\", protein_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d362c7-3197-4107-9efc-666eebcc06d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '200_node_network_embeddings_latest.csv' \n",
    "node_dataset = pd.read_csv(data_path, index_col=0)\n",
    "node_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ccdc18-d6b7-44dc-847b-3cd626d9dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "xcell_path = \"/home/user/Diamond_101225/diamond_rho/diamond_tpm/gene_xcell_spearman_rho_with_ENSG.csv\"\n",
    "print(\"Loading xCell PCs from:\", xcell_path)\n",
    "# separator detection\n",
    "xcell = pd.read_csv(xcell_path, sep=None, engine='python')\n",
    "\n",
    "if 'gene_id' in xcell.columns:\n",
    "    gene_col = 'gene_id'\n",
    "    xcell['ensembl'] = xcell['gene_id'].astype(str).str.split('.').str[0]\n",
    "else:\n",
    "    gene_col = xcell.columns[0]\n",
    "    xcell['ensembl'] = xcell[gene_col].astype(str).str.split('.').str[0]\n",
    "pc_cols = [c for c in xcell.columns if ('xcell' in str(c).lower() and 'pc' in str(c).lower()) or str(c).lower().startswith('xcell_pc')]\n",
    "\n",
    "if len(pc_cols) < 1:\n",
    "    possible = [c for c in xcell.columns if c not in (gene_col, 'Gene', 'ensembl')]\n",
    "    pc_cols = possible[:20]\n",
    "\n",
    "print(\"Detected xCell PC columns (count={}): {}\".format(len(pc_cols), pc_cols[:30]))\n",
    "\n",
    "if len(pc_cols) == 0:\n",
    "    print(\"WARNING: no xCell PC columns detected. Please inspect file and set pc_cols manually.\")\n",
    "else:\n",
    "    xcell_idx = xcell.set_index('ensembl')[pc_cols]\n",
    "\n",
    "    # Alignment\n",
    "    xcell_aligned = xcell_idx.reindex(node_dataset.index)\n",
    "\n",
    "    missing_pct = xcell_aligned.isna().mean(axis=0).round(3) * 100\n",
    "    print(\"Percent missing per xCell PC column:\\n\", missing_pct)\n",
    "\n",
    "    # Missing values\n",
    "    fill_strategy = \"mean\"  # options: \"mean\" or \"zero\"\n",
    "    if fill_strategy == \"zero\":\n",
    "        xcell_aligned_filled = xcell_aligned.fillna(0.0)\n",
    "    else:\n",
    "        xcell_aligned_filled = xcell_aligned.fillna(xcell_aligned.mean())\n",
    "\n",
    "    # renaming\n",
    "    xcell_aligned_filled.columns = [f\"xcell_{str(c)}\" if not str(c).startswith(\"xcell_\") else str(c) for c in xcell_aligned_filled.columns]\n",
    "\n",
    "    # concatenation\n",
    "    node_dataset = pd.concat([node_dataset, xcell_aligned_filled], axis=1)\n",
    "\n",
    "    # merging\n",
    "    out_merge = \"node_dataset_with_xcell_rho.csv\"\n",
    "    node_dataset.to_csv(out_merge)\n",
    "    print(\"Saved merged node_dataset to:\", out_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc03370d-ec27-4861-bcc0-0b82a2a938a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dataset.sort_index(inplace=True)\n",
    "node_dataset.reset_index(drop=False, inplace=True)\n",
    "assert((node_dataset.index.to_numpy()==np.arange(len(node_dataset))).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92948962-eeea-4704-a658-d0cabd873193",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name = 'my_label'\n",
    "\n",
    "pos_label_col = 'gda_score' \n",
    "node_dataset[pos_label_col].fillna(0, inplace=True) # Replace NaN values with 0  # Replace NaN values with 0\n",
    "pos_labels = pd.array([1 if row[pos_label_col] else None for id_, row in node_dataset.iterrows()], dtype='Int32')\n",
    "node_dataset[label_name] = pos_labels\n",
    "\n",
    "def sample_negatives(PU_labels):\n",
    "    '''randomly samples from the unlabeled samples'''\n",
    "\n",
    "    # sample same # as positives\n",
    "    num_pos = (PU_labels==1).sum()\n",
    "    neg_inds = PU_labels[PU_labels.isna()].sample(num_pos).index\n",
    "\n",
    "    return neg_inds \n",
    "\n",
    "neg_label_inds = sample_negatives(node_dataset[label_name])\n",
    "node_dataset.loc[neg_label_inds, label_name] = 0\n",
    "\n",
    "node_dataset[label_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e0aa0-1662-44b3-ae21-de57bf942956",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dataset.set_index(node_dataset.columns[0], inplace=True)\n",
    "node_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57118e97-29e4-454c-ac27-286799fa7a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = node_dataset.columns[-1]\n",
    "node_dataset[label_col] = node_dataset[label_col].astype('Int32')\n",
    "\n",
    "if 'gda_score' in node_dataset.columns:\n",
    "    node_dataset = node_dataset.drop(columns=['gda_score'])\n",
    "\n",
    "node_feat_cols = node_dataset.columns[:-1].tolist()\n",
    "\n",
    "node_data = node_dataset[node_feat_cols + [label_col]]\n",
    "\n",
    "X = torch.Tensor(node_data[node_feat_cols].select_dtypes(include=[np.number]).to_numpy(dtype=np.float32))\n",
    "\n",
    "y = node_data[label_col].fillna(-1).astype('int')\n",
    "y = torch.Tensor(y).type(torch.int64)\n",
    "\n",
    "node_data_labeled = node_data[node_data[label_col].notna()]\n",
    "node_data_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c5641-a1ea-4204-8690-0c8f412f8ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "X_myIDs = node_data_labeled.index.to_numpy()\n",
    "labels = node_data_labeled[label_col].to_numpy()\n",
    "\n",
    "test_size = 0.2\n",
    "val_size = 0.1 * (1 / (1 - test_size))\n",
    "\n",
    "myIDs_train_val, myIDs_test = train_test_split(X_myIDs, test_size=test_size, shuffle=True, stratify=labels)\n",
    "\n",
    "labels_train_val = node_data_labeled.loc[myIDs_train_val, label_col].to_numpy()\n",
    "myIDs_train, myIDs_val = train_test_split(myIDs_train_val, test_size=val_size, shuffle=True, stratify=labels_train_val)\n",
    "\n",
    "id_to_idx = {id_: idx for idx, id_ in enumerate(node_data.index)}\n",
    "\n",
    "train_idx = np.array([id_to_idx[i] for i in myIDs_train])\n",
    "val_idx = np.array([id_to_idx[i] for i in myIDs_val])\n",
    "test_idx = np.array([id_to_idx[i] for i in myIDs_test])\n",
    "\n",
    "n_nodes = len(node_data)\n",
    "\n",
    "train_mask = np.zeros(n_nodes, dtype=bool)\n",
    "train_mask[train_idx] = True\n",
    "train_mask = torch.tensor(train_mask, dtype=torch.bool)\n",
    "\n",
    "val_mask = np.zeros(n_nodes, dtype=bool)\n",
    "val_mask[val_idx] = True\n",
    "val_mask = torch.tensor(val_mask, dtype=torch.bool)\n",
    "\n",
    "test_mask = np.zeros(n_nodes, dtype=bool)\n",
    "test_mask[test_idx] = True\n",
    "test_mask = torch.tensor(test_mask, dtype=torch.bool)\n",
    "\n",
    "print(f\"Number of training nodes: {train_mask.sum().item()}\")\n",
    "print(f\"Number of validation nodes: {val_mask.sum().item()}\")\n",
    "print(f\"Number of test nodes: {test_mask.sum().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a81f1b7-536c-4ee8-963d-5af6fd509554",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(x=X, y=y, edge_index=edge_list, edge_attr=edge_weight)\n",
    "\n",
    "num_classes = 2\n",
    "num_features = X.shape[1]\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask\n",
    "data.test_mask = test_mask\n",
    "assert y.shape[0] == X.shape[0], \"Mismatch: y and X must have the same number of nodes\"\n",
    "\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790a5310-728f-4ecf-9f12-489f8968617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"\"#WANDB API key\n",
    "os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "\n",
    "config = {\n",
    "    \"dataset\": \"CIFAR10\",\n",
    "    \"machine\": \"online cluster\",\n",
    "    \"model\": \"CNN\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "\n",
    "wandb.init(project=\"offline-demo\")\n",
    "\n",
    "for i in range(100):\n",
    "    wandb.log({\"accuracy\": i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f7da58-8674-490f-a7cd-9c5c716a17d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accs, train_precs, train_recalls, train_f1s = [], [], [], []\n",
    "train_aurocs, train_auprcs = [], []\n",
    "\n",
    "test_accs, test_precs, test_recalls, test_f1s = [], [], [], []\n",
    "test_aurocs, test_auprcs = [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe64f7-e028-4a6f-84a2-00f75328cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, SGConv, MessagePassing\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "class GNNModel(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes, hidden_dense, GNN_conv_layer=GCNConv, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.GNN_conv_layer = GNN_conv_layer\n",
    "\n",
    "        self.convs.append(GNN_conv_layer(in_channels=num_features, out_channels=hidden_channels[0], **kwargs))\n",
    "\n",
    "        for c1, c2 in zip(hidden_channels[:-1], hidden_channels[1:]):\n",
    "            self.convs.append(GNN_conv_layer(in_channels=c1, out_channels=c2, **kwargs))\n",
    "\n",
    "        self.dense1 = torch.nn.Linear(hidden_channels[-1], hidden_dense)\n",
    "        self.dense_out = torch.nn.Linear(hidden_dense, num_classes)\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        for conv in self.convs:\n",
    "            if isinstance(conv, SGConv):\n",
    "                x = conv(x, edge_index, edge_weight=edge_weight)\n",
    "            else:\n",
    "                x = conv(x, edge_index, edge_weight=edge_weight)\n",
    "            x = x.relu()\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = self.dense_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class WeightedSAGEConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(aggr='mean')\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.lin_update = torch.nn.Linear(in_channels + out_channels, out_channels)\n",
    "        self.last_messages = None\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        self.log(\"edge_shape\", edge_index.shape[1], on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        print(f\"Edge shape: {edge_index.shape}\") \n",
    "\n",
    "        x_transformed = self.lin(x)\n",
    "        self.last_messages = None\n",
    "        out = self.propagate(edge_index=edge_index, x=x_transformed, edge_weight=edge_weight)\n",
    "        out = self.lin_update(torch.cat([out, x], dim=1))\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, edge_weight):\n",
    "        messages = edge_weight.view(-1, 1) * x_j\n",
    "        self.last_messages = messages.detach().cpu()\n",
    "        return messages\n",
    "\n",
    "class LitGNN(pl.LightningModule):\n",
    "    def __init__(self, model_name, num_features, hidden_channels, num_classes, hidden_dense, GNN_conv_layer, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters() \n",
    "        self.model_name = model_name\n",
    "        self.model = GNNModel(num_features, hidden_channels, num_classes, hidden_dense, GNN_conv_layer, dropout_rate)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        return self.model(x, edge_index, edge_weight)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        out = self(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        valid_indices = batch.y >= 0\n",
    "        \n",
    "        self.log(\"edge_shape\", batch.edge_index.shape[1])\n",
    "        print(f\"Training Edge shape: {batch.edge_index.shape}\")\n",
    "        \n",
    "        if valid_indices.any():\n",
    "            loss = self.criterion(out[valid_indices], batch.y[valid_indices])\n",
    "        else:\n",
    "            loss = torch.tensor(0.0, requires_grad=True, device=self.device)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        out = self(batch.x, batch.edge_index, edge_weight=batch.edge_attr)\n",
    "        valid_indices = batch.y >= 0\n",
    "\n",
    "        if valid_indices.any():\n",
    "            loss = self.criterion(out[valid_indices], batch.y[valid_indices])\n",
    "            acc = (out[valid_indices].argmax(dim=1) == batch.y[valid_indices]).float().mean()\n",
    "        else:\n",
    "            loss = torch.tensor(0.0, device=self.device)\n",
    "            acc = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, on_epoch=True, prog_bar=True)\n",
    "        self.validation_step_outputs.append({\"val_loss\": loss, \"val_acc\": acc})\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        val_acc_mean = torch.stack([x[\"val_acc\"] for x in self.validation_step_outputs]).mean()\n",
    "        self.log(\"val_acc_epoch\", val_acc_mean, prog_bar=True)\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        out = self(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        valid_indices = batch.y >= 0\n",
    "\n",
    "        if valid_indices.any():\n",
    "            loss = self.criterion(out[valid_indices], batch.y[valid_indices])\n",
    "            acc = (out[valid_indices].argmax(dim=1) == batch.y[valid_indices]).float().mean()\n",
    "        else:\n",
    "            loss = torch.tensor(0.0, device=self.device)\n",
    "            acc = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, on_epoch=True, prog_bar=True)\n",
    "        return {\"test_loss\": loss, \"test_acc\": acc}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        test_acc_mean = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\n",
    "        self.log(\"test_acc_epoch\", test_acc_mean, prog_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77885dc0-ddac-4564-8ffd-0d0fdcd071c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, ModelSummary, EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import datetime\n",
    "\n",
    "AVAIL_GPUS = 0 \n",
    "MAX_EPOCHS = 200 \n",
    "NUM_TRIALS = 25 \n",
    "GNN_conv_layer=WeightedSAGEConv \n",
    "\n",
    "# Loop through trials\n",
    "for trial in range(1, NUM_TRIALS + 1):\n",
    "    model_name = f'gat_{datetime.datetime.today().strftime(\"%Y-%m-%d\")}_trial_{trial}'\n",
    "    logger = WandbLogger(name=model_name, project=\"\", log_model=\"all\") #Project name\n",
    "\n",
    "    model = LitGNN(\n",
    "        model_name=\"SGConvNet\", \n",
    "        num_features=num_features, \n",
    "        hidden_channels=[128], \n",
    "        num_classes=num_classes, \n",
    "        hidden_dense=64, \n",
    "        GNN_conv_layer=SGConv, \n",
    "        dropout_rate=0.4\n",
    "    )\n",
    "\n",
    "    train_data_loader = DataLoader([data], batch_size=1, num_workers=3)\n",
    "    val_data_loader = DataLoader([data], batch_size=1, num_workers=3)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        save_weights_only=True,\n",
    "        mode=\"max\", \n",
    "        monitor=\"val_acc\",\n",
    "        dirpath=f\"checkpoints/trial_{trial}\",\n",
    "        filename=\"{epoch:02d}-{val_acc:.2f}\"\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        callbacks=[\n",
    "            checkpoint_callback, \n",
    "            EarlyStopping(monitor=\"val_acc\", patience=20, verbose=True, mode=\"max\"),\n",
    "            ModelSummary(max_depth=3)\n",
    "        ],\n",
    "        devices=1, \n",
    "        accelerator=\"cpu\",\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_dataloaders=train_data_loader, val_dataloaders=val_data_loader)\n",
    "\n",
    "    best_model_path = checkpoint_callback.best_model_path\n",
    "    if best_model_path:\n",
    "        model = LitGNN.load_from_checkpoint(\n",
    "            best_model_path,\n",
    "            model_name=\"SGConvNet\",\n",
    "            num_features=num_features,\n",
    "            hidden_channels=[128],\n",
    "            num_classes=num_classes,\n",
    "            hidden_dense=64,\n",
    "            GNN_conv_layer=SGConv,\n",
    "            dropout_rate=0.4\n",
    "        )\n",
    "        print(f\"Trial {trial}: Loaded model from checkpoint: {best_model_path}\")\n",
    "    else:\n",
    "        print(f\"Trial {trial}: No checkpoint found. Training from scratch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f051765-18e6-4827-b566-73e46ce7b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "\n",
    "def to_numpy(x):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x\n",
    "    return x.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "train_accuracies, train_precisions, train_recalls, train_f1s = [], [], [], []\n",
    "test_accuracies, test_precisions, test_recalls, test_f1s = [], [], [], []\n",
    "\n",
    "train_aucs, test_aucs = [], []\n",
    "train_auprs, test_auprs = [], []\n",
    "\n",
    "\n",
    "for trial in range(1, NUM_TRIALS + 1):\n",
    "    print(f\"\\nEvaluating Trial {trial}\")\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    data = data.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(\n",
    "            data.x,\n",
    "            data.edge_index,\n",
    "            data.edge_attr\n",
    "        )\n",
    "\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "\n",
    "    train_mask = data.train_mask\n",
    "    y_train = data.y[train_mask]\n",
    "    preds_train = logits[train_mask].argmax(dim=1)\n",
    "    probs_train = probs[train_mask][:, 1]\n",
    "\n",
    "    train_report = classification_report(\n",
    "        to_numpy(y_train),\n",
    "        to_numpy(preds_train),\n",
    "        labels=[0, 1],\n",
    "        target_names=[\"negative\", \"positive\"],\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    train_accuracies.append(train_report[\"accuracy\"])\n",
    "    train_precisions.append(train_report[\"positive\"][\"precision\"])\n",
    "    train_recalls.append(train_report[\"positive\"][\"recall\"])\n",
    "    train_f1s.append(train_report[\"positive\"][\"f1-score\"])\n",
    "\n",
    "    if len(torch.unique(y_train)) > 1:\n",
    "        train_aucs.append(\n",
    "            roc_auc_score(to_numpy(y_train), to_numpy(probs_train))\n",
    "        )\n",
    "        train_auprs.append(\n",
    "            average_precision_score(to_numpy(y_train), to_numpy(probs_train))\n",
    "        )\n",
    "    else:\n",
    "        train_aucs.append(np.nan)\n",
    "        train_auprs.append(np.nan)\n",
    "\n",
    "    test_mask = data.test_mask\n",
    "    y_test = data.y[test_mask]\n",
    "    preds_test = logits[test_mask].argmax(dim=1)\n",
    "    probs_test = probs[test_mask][:, 1]\n",
    "\n",
    "    test_report = classification_report(\n",
    "        to_numpy(y_test),\n",
    "        to_numpy(preds_test),\n",
    "        labels=[0, 1],\n",
    "        target_names=[\"negative\", \"positive\"],\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    test_accuracies.append(test_report[\"accuracy\"])\n",
    "    test_precisions.append(test_report[\"positive\"][\"precision\"])\n",
    "    test_recalls.append(test_report[\"positive\"][\"recall\"])\n",
    "    test_f1s.append(test_report[\"positive\"][\"f1-score\"])\n",
    "\n",
    "    if len(torch.unique(y_test)) > 1:\n",
    "        test_aucs.append(\n",
    "            roc_auc_score(to_numpy(y_test), to_numpy(probs_test))\n",
    "        )\n",
    "        test_auprs.append(\n",
    "            average_precision_score(to_numpy(y_test), to_numpy(probs_test))\n",
    "        )\n",
    "    else:\n",
    "        test_aucs.append(np.nan)\n",
    "        test_auprs.append(np.nan)\n",
    "\n",
    "\n",
    "print(\"\\nAggregated Metrics (Mean ± Std)\")\n",
    "\n",
    "print(f\"Train Accuracy : {np.mean(train_accuracies):.4f} ± {np.std(train_accuracies):.4f}\")\n",
    "print(f\"Train Precision: {np.mean(train_precisions):.4f} ± {np.std(train_precisions):.4f}\")\n",
    "print(f\"Train Recall   : {np.mean(train_recalls):.4f} ± {np.std(train_recalls):.4f}\")\n",
    "print(f\"Train F1       : {np.mean(train_f1s):.4f} ± {np.std(train_f1s):.4f}\")\n",
    "print(f\"Train AUROC    : {np.nanmean(train_aucs):.4f} ± {np.nanstd(train_aucs):.4f}\")\n",
    "print(f\"Train AUPRC    : {np.nanmean(train_auprs):.4f} ± {np.nanstd(train_auprs):.4f}\")\n",
    "\n",
    "print(f\"Test Accuracy  : {np.mean(test_accuracies):.4f} ± {np.std(test_accuracies):.4f}\")\n",
    "print(f\"Test Precision : {np.mean(test_precisions):.4f} ± {np.std(test_precisions):.4f}\")\n",
    "print(f\"Test Recall    : {np.mean(test_recalls):.4f} ± {np.std(test_recalls):.4f}\")\n",
    "print(f\"Test F1        : {np.mean(test_f1s):.4f} ± {np.std(test_f1s):.4f}\")\n",
    "print(f\"Test AUROC     : {np.nanmean(test_aucs):.4f} ± {np.nanstd(test_aucs):.4f}\")\n",
    "print(f\"Test AUPRC     : {np.nanmean(test_auprs):.4f} ± {np.nanstd(test_auprs):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
