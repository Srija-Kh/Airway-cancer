{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b29a186-6767-446b-8436-45638fcee853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(314159) # set random seed\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6677f0f4-6126-44a3-9bcf-23ba769b1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load the edge list\n",
    "edge_list = np.load(\"edge_list_latest1.npy\", allow_pickle=True)\n",
    "\n",
    "edge_list = np.array([row[0].split(\",\") for row in edge_list[1:]])\n",
    "\n",
    "# Extract the source nodes, target nodes, and combined scores\n",
    "protein1 = edge_list[:, 0] \n",
    "protein2 = edge_list[:, 1] \n",
    "combined_score = edge_list[:, 2].astype(np.float32)\n",
    "\n",
    "# Mapping\n",
    "protein_map = {protein: idx for idx, protein in enumerate(np.unique(np.concatenate((protein1, protein2))))}\n",
    "protein1 = np.vectorize(protein_map.get)(protein1)\n",
    "protein2 = np.vectorize(protein_map.get)(protein2)\n",
    "\n",
    "# Stacking\n",
    "edges = np.stack([protein1, protein2], axis=0)\n",
    "edge_list = torch.tensor(edges, dtype=torch.long)  # Edge index tensor\n",
    "edge_weight = torch.tensor(combined_score, dtype=torch.float32)  # Edge weight tensor\n",
    "\n",
    "print(\"Edge list shape:\", edge_list.shape)  # Should be [2, n_edges]\n",
    "print(\"Edge weights shape:\", edge_weight.shape)  # Should be [n_edges]\n",
    "print(\"Protein map:\", protein_map)  # Mapping of protein names to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84022278-b785-4bed-bd68-3189b35db60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '200_node_network_embeddings_latest.csv'# Network embeddings\n",
    "node_dataset = pd.read_csv(data_path, index_col=0)\n",
    "node_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f9b9b5-4f7e-4fbf-bed0-2d4344576cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xcell_path = \"/home/user/Diamond_101225/diamond_rho/diamond_tpm/gene_xcell_spearman_rho_with_ENSG.csv\"\n",
    "print(\"Loading xCell PCs from:\", xcell_path)\n",
    "xcell = pd.read_csv(xcell_path, sep=None, engine='python')\n",
    "if 'gene_id' in xcell.columns:\n",
    "    gene_col = 'gene_id'\n",
    "    xcell['ensembl'] = xcell['gene_id'].astype(str).str.split('.').str[0]\n",
    "else:\n",
    "    gene_col = xcell.columns[0]\n",
    "    xcell['ensembl'] = xcell[gene_col].astype(str).str.split('.').str[0]\n",
    "\n",
    "pc_cols = [c for c in xcell.columns if ('xcell' in str(c).lower() and 'pc' in str(c).lower()) or str(c).lower().startswith('xcell_pc')]\n",
    "\n",
    "if len(pc_cols) < 1:\n",
    "    possible = [c for c in xcell.columns if c not in (gene_col, 'Gene', 'ensembl')]\n",
    "    pc_cols = possible[:20]\n",
    "\n",
    "print(\"xCell PC columns (count={}): {}\".format(len(pc_cols), pc_cols[:30]))\n",
    "\n",
    "if len(pc_cols) == 0:\n",
    "    print(\"no xCell PC columns detected.\")\n",
    "\n",
    "else:\n",
    "    xcell_idx = xcell.set_index('ensembl')[pc_cols]\n",
    "\n",
    "    # Alignment\n",
    "    xcell_aligned = xcell_idx.reindex(node_dataset.index)\n",
    "\n",
    "    missing_pct = xcell_aligned.isna().mean(axis=0).round(3) * 100\n",
    "    print(\"Percent missing per xCell PC column:\\n\", missing_pct)\n",
    "\n",
    "    fill_strategy = \"mean\" \n",
    "    if fill_strategy == \"zero\":\n",
    "        xcell_aligned_filled = xcell_aligned.fillna(0.0)\n",
    "    else:\n",
    "        xcell_aligned_filled = xcell_aligned.fillna(xcell_aligned.mean())\n",
    "\n",
    "    xcell_aligned_filled.columns = [f\"xcell_{str(c)}\" if not str(c).startswith(\"xcell_\") else str(c) for c in xcell_aligned_filled.columns]\n",
    "\n",
    "    # concatenation\n",
    "    node_dataset = pd.concat([node_dataset, xcell_aligned_filled], axis=1)\n",
    "\n",
    "    # savinh\n",
    "    out_merge = \"node_dataset_with_xcell_rho.csv\"\n",
    "    node_dataset.to_csv(out_merge)\n",
    "    print(\"Saved merged node_dataset to:\", out_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b7f90d-037b-440e-afca-ce234f7a5e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dataset.sort_index(inplace=True)\n",
    "node_dataset.reset_index(drop=False, inplace=True)\n",
    "assert((node_dataset.index.to_numpy()==np.arange(len(node_dataset))).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe71a5-6bfc-44b7-8bcf-bd739d6a5e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name = 'my_label'\n",
    "\n",
    "# find positives\n",
    "pos_label_col = 'gda_score' \n",
    "node_dataset[pos_label_col].fillna(0, inplace=True) \n",
    "pos_labels = pd.array([1 if row[pos_label_col] else None for id_, row in node_dataset.iterrows()], dtype='Int32')\n",
    "node_dataset[label_name] = pos_labels\n",
    "\n",
    "def sample_negatives(PU_labels):\n",
    "    '''randomly samples from the unlabeled samples'''\n",
    "    num_pos = (PU_labels==1).sum()\n",
    "    neg_inds = PU_labels[PU_labels.isna()].sample(num_pos).index\n",
    "\n",
    "    return neg_inds\n",
    "\n",
    "neg_label_inds = sample_negatives(node_dataset[label_name])\n",
    "node_dataset.loc[neg_label_inds, label_name] = 0\n",
    "\n",
    "node_dataset[label_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b32ccfd-5e74-410f-b3a4-352d66276ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dataset.set_index(node_dataset.columns[0], inplace=True)\n",
    "\n",
    "node_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3620fc-d680-4d6e-9f94-cd671e4e441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = node_dataset.columns[-1] \n",
    "node_dataset[label_col] = node_dataset[label_col].astype('Int32')\n",
    "\n",
    "# Remove the 'gda_score' column\n",
    "if 'gda_score' in node_dataset.columns:\n",
    "    node_dataset = node_dataset.drop(columns=['gda_score'])\n",
    "\n",
    "node_feat_cols = node_dataset.columns[:-1].tolist()  # All columns except the label\n",
    "\n",
    "\n",
    "# Get subset of node features + labels\n",
    "node_data = node_dataset[node_feat_cols + [label_col]]\n",
    "\n",
    "# Convert features to PyTorch Tensor\n",
    "X = torch.Tensor(node_data[node_feat_cols].select_dtypes(include=[np.number]).to_numpy(dtype=np.float32))\n",
    "\n",
    "\n",
    "# Convert labels to PyTorch Tensor, filling NaN with -1\n",
    "y = node_data[label_col].fillna(-1).astype('int')\n",
    "y = torch.Tensor(y).type(torch.int64)\n",
    "\n",
    "# Restrict to data with labels (non-NaN)\n",
    "node_data_labeled = node_data[node_data[label_col].notna()]\n",
    "node_data_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5afae0-4a65-4666-9958-930c1cfbbd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Extract the labeled node indices and labels\n",
    "X_myIDs = node_data_labeled.index.to_numpy() \n",
    "labels = node_data_labeled[label_col].to_numpy() \n",
    "\n",
    "test_size = 0.2\n",
    "val_size = 0.1 * (1 / (1 - test_size)) \n",
    "\n",
    "# Perform stratified train-test split\n",
    "myIDs_train_val, myIDs_test = train_test_split(X_myIDs, test_size=test_size, shuffle=True, stratify=labels)\n",
    "\n",
    "# Perform stratified train-validation split\n",
    "labels_train_val = node_data_labeled.loc[myIDs_train_val, label_col].to_numpy()\n",
    "myIDs_train, myIDs_val = train_test_split(myIDs_train_val, test_size=val_size, shuffle=True, stratify=labels_train_val)\n",
    "\n",
    "# Convert Ensembl IDs to Integer-Based Indices\n",
    "# Create a mapping from Ensembl ID to row position\n",
    "id_to_idx = {id_: idx for idx, id_ in enumerate(node_data.index)}\n",
    "\n",
    "# Map train, val, and test IDs to their corresponding row indices\n",
    "train_idx = np.array([id_to_idx[i] for i in myIDs_train])\n",
    "val_idx = np.array([id_to_idx[i] for i in myIDs_val])\n",
    "test_idx = np.array([id_to_idx[i] for i in myIDs_test])\n",
    "\n",
    "# Create boolean masks\n",
    "n_nodes = len(node_data)\n",
    "\n",
    "train_mask = np.zeros(n_nodes, dtype=bool)\n",
    "train_mask[train_idx] = True\n",
    "train_mask = torch.tensor(train_mask, dtype=torch.bool)\n",
    "\n",
    "val_mask = np.zeros(n_nodes, dtype=bool)\n",
    "val_mask[val_idx] = True\n",
    "val_mask = torch.tensor(val_mask, dtype=torch.bool)\n",
    "\n",
    "test_mask = np.zeros(n_nodes, dtype=bool)\n",
    "test_mask[test_idx] = True\n",
    "test_mask = torch.tensor(test_mask, dtype=torch.bool)\n",
    "\n",
    "# Print distributions\n",
    "print(f\"Number of training nodes: {train_mask.sum().item()}\")\n",
    "print(f\"Number of validation nodes: {val_mask.sum().item()}\")\n",
    "print(f\"Number of test nodes: {test_mask.sum().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d12100b-f5c3-4086-bec6-99465199082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(x=X, y=y, edge_index=edge_list, edge_attr=edge_weight)  # Add edge weights\n",
    "\n",
    "num_classes = 2\n",
    "num_features = X.shape[1]\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask\n",
    "data.test_mask = test_mask\n",
    "assert y.shape[0] == X.shape[0], \"Mismatch: y and X must have the same number of nodes\"\n",
    "\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d61476b-49ed-4241-9aca-0ea6e7019ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"\" #WANDB API KEY\n",
    "os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "\n",
    "config = {\n",
    "    \"dataset\": \"CIFAR10\",\n",
    "    \"machine\": \"online cluster\",\n",
    "    \"model\": \"CNN\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "\n",
    "wandb.init(project=\"offline-demo\")\n",
    "\n",
    "for i in range(100):\n",
    "    wandb.log({\"accuracy\": i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed43069-e5e8-479e-a2f7-ff74a5313755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    ModelSummary,\n",
    "    EarlyStopping\n",
    ")\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import datetime\n",
    "class WeightedSAGEConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(aggr=\"mean\")\n",
    "\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.lin_update = torch.nn.Linear(in_channels + out_channels, out_channels)\n",
    "\n",
    "        self.last_messages = None \n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        x_transformed = self.lin(x)\n",
    "\n",
    "        out = self.propagate(\n",
    "            edge_index=edge_index,\n",
    "            x=x_transformed,\n",
    "            edge_weight=edge_weight\n",
    "        )\n",
    "\n",
    "        out = self.lin_update(torch.cat([out, x], dim=1))\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, edge_weight):\n",
    "        messages = edge_weight.view(-1, 1) * x_j\n",
    "        self.last_messages = messages.detach().cpu()\n",
    "        return messages\n",
    "class GNNModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features,\n",
    "        hidden_channels,\n",
    "        num_classes,\n",
    "        hidden_dense,\n",
    "        GNN_conv_layer,\n",
    "        dropout_rate=0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GNN_conv_layer(num_features, hidden_channels[0]))\n",
    "\n",
    "        for c1, c2 in zip(hidden_channels[:-1], hidden_channels[1:]):\n",
    "            self.convs.append(GNN_conv_layer(c1, c2))\n",
    "\n",
    "        self.dense1 = torch.nn.Linear(hidden_channels[-1], hidden_dense)\n",
    "        self.dense_out = torch.nn.Linear(hidden_dense, num_classes)\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, edge_weight=edge_weight)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = self.dense_out(x)\n",
    "\n",
    "        return x\n",
    "class LitGNN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        num_features,\n",
    "        hidden_channels,\n",
    "        num_classes,\n",
    "        hidden_dense,\n",
    "        GNN_conv_layer,\n",
    "        dropout_rate,\n",
    "        lr=1e-3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = GNNModel(\n",
    "            num_features=num_features,\n",
    "            hidden_channels=hidden_channels,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dense=hidden_dense,\n",
    "            GNN_conv_layer=GNN_conv_layer,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def forward(self, batch, x_override=None):\n",
    "        x = x_override if x_override is not None else batch.x\n",
    "        return self.model(x, batch.edge_index, batch.edge_attr)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        out = self(batch)\n",
    "        valid = batch.y >= 0\n",
    "\n",
    "        loss = (\n",
    "            self.criterion(out[valid], batch.y[valid])\n",
    "            if valid.any()\n",
    "            else torch.tensor(0.0, device=self.device, requires_grad=True)\n",
    "        )\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        out = self(batch)\n",
    "        valid = batch.y >= 0\n",
    "\n",
    "        if valid.any():\n",
    "            loss = self.criterion(out[valid], batch.y[valid])\n",
    "            acc = (out[valid].argmax(dim=1) == batch.y[valid]).float().mean()\n",
    "        else:\n",
    "            loss = torch.tensor(0.0, device=self.device)\n",
    "            acc = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        self.validation_step_outputs.append(acc)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.validation_step_outputs:\n",
    "            mean_acc = torch.stack(self.validation_step_outputs).mean()\n",
    "            self.log(\"val_acc_epoch\", mean_acc, prog_bar=True)\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        out = self(batch)\n",
    "        valid = batch.y >= 0\n",
    "\n",
    "        if valid.any():\n",
    "            loss = self.criterion(out[valid], batch.y[valid])\n",
    "            acc = (out[valid].argmax(dim=1) == batch.y[valid]).float().mean()\n",
    "        else:\n",
    "            loss = torch.tensor(0.0, device=self.device)\n",
    "            acc = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        self.log(\"test_loss\", loss, on_epoch=True)\n",
    "        self.log(\"test_acc\", acc, on_epoch=True)\n",
    "        return acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c641f-24e2-4134-83d3-8502dce236a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, ModelSummary, EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import datetime\n",
    "\n",
    "AVAIL_GPUS = 0\n",
    "MAX_EPOCHS = 200\n",
    "NUM_TRIALS = 25\n",
    "\n",
    "GNN_conv_layer = WeightedSAGEConv\n",
    "\n",
    "for trial in range(1, NUM_TRIALS + 1):\n",
    "\n",
    "    model_name = f\"graphsage_{datetime.datetime.today().strftime('%Y-%m-%d')}_trial_{trial}\"\n",
    "\n",
    "    logger = WandbLogger(\n",
    "        name=model_name,\n",
    "        project=\"\",\n",
    "        log_model=\"all\",\n",
    "    )\n",
    "\n",
    "    model = LitGNN(\n",
    "        model_name=\"GraphSAGE\",\n",
    "        num_features=num_features,\n",
    "        hidden_channels=[128],\n",
    "        num_classes=num_classes,\n",
    "        hidden_dense=64,\n",
    "        GNN_conv_layer=GNN_conv_layer,\n",
    "        dropout_rate=0.3,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader([data], batch_size=1, num_workers=0)\n",
    "    val_loader = DataLoader([data], batch_size=1, num_workers=0)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        save_weights_only=True,\n",
    "        monitor=\"val_acc\",\n",
    "        mode=\"max\",\n",
    "        dirpath=f\"checkpoints/trial_{trial}\",\n",
    "        filename=\"{epoch:02d}-{val_acc:.3f}\",\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"cpu\",\n",
    "        devices=1,\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        logger=logger,\n",
    "        callbacks=[\n",
    "            checkpoint_callback,\n",
    "            EarlyStopping(monitor=\"val_acc\", patience=20, mode=\"max\"),\n",
    "            ModelSummary(max_depth=3),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    if checkpoint_callback.best_model_path:\n",
    "        model = LitGNN.load_from_checkpoint(\n",
    "            checkpoint_callback.best_model_path,\n",
    "            model_name=\"GraphSAGE\",\n",
    "            num_features=num_features,\n",
    "            hidden_channels=[128],\n",
    "            num_classes=num_classes,\n",
    "            hidden_dense=64,\n",
    "            GNN_conv_layer=GNN_conv_layer,\n",
    "            dropout_rate=0.3,\n",
    "        )\n",
    "\n",
    "        print(f\"Trial {trial}: loaded {checkpoint_callback.best_model_path}\")\n",
    "    else:\n",
    "        print(f\"Trial {trial}: no checkpoint found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febbb8e9-0d5d-4748-9194-db925cf2ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "train_accuracies, train_precisions, train_recalls, train_f1s = [], [], [], []\n",
    "test_accuracies, test_precisions, test_recalls, test_f1s = [], [], [], []\n",
    "\n",
    "for trial in range(1, NUM_TRIALS + 1):\n",
    "    print(f\"\\nEvaluating Trial {trial}\")\n",
    "\n",
    "    data = data.to(device='cpu') \n",
    "\n",
    "    logits = model(data)\n",
    "    preds_train = logits[data.train_mask].argmax(dim=-1)\n",
    "    preds_test = logits[data.test_mask].argmax(dim=-1)\n",
    "\n",
    "    y_train = data.y[data.train_mask]\n",
    "    y_test = data.y[data.test_mask]\n",
    "\n",
    "    train_report = classification_report(\n",
    "        y_train.cpu(), preds_train.cpu(), labels=[0, 1], target_names=['negative', 'positive'], output_dict=True\n",
    "    )\n",
    "    test_report = classification_report(\n",
    "        y_test.cpu(), preds_test.cpu(), labels=[0, 1], target_names=['negative', 'positive'], output_dict=True\n",
    "    )\n",
    "\n",
    "    train_accuracies.append(train_report['accuracy'])\n",
    "    train_precisions.append(train_report['positive']['precision'])\n",
    "    train_recalls.append(train_report['positive']['recall'])\n",
    "    train_f1s.append(train_report['positive']['f1-score'])\n",
    "    \n",
    "    test_accuracies.append(test_report['accuracy'])\n",
    "    test_precisions.append(test_report['positive']['precision'])\n",
    "    test_recalls.append(test_report['positive']['recall'])\n",
    "    test_f1s.append(test_report['positive']['f1-score'])\n",
    "\n",
    "    print(f\"Trial {trial} Training Metrics:\\n{train_report}\")\n",
    "    print(f\"Trial {trial} Testing Metrics:\\n{test_report}\")\n",
    "\n",
    "print(\"\\nAggregated Metrics (Mean ± Std):\")\n",
    "print(f\"Training Accuracy: {np.mean(train_accuracies):.4f} ± {np.std(train_accuracies):.4f}\")\n",
    "print(f\"Training Precision: {np.mean(train_precisions):.4f} ± {np.std(train_precisions):.4f}\")\n",
    "print(f\"Training Recall: {np.mean(train_recalls):.4f} ± {np.std(train_recalls):.4f}\")\n",
    "print(f\"Training F1-Score: {np.mean(train_f1s):.4f} ± {np.std(train_f1s):.4f}\")\n",
    "print(f\"Testing Accuracy: {np.mean(test_accuracies):.4f} ± {np.std(test_accuracies):.4f}\")\n",
    "print(f\"Testing Precision: {np.mean(test_precisions):.4f} ± {np.std(test_precisions):.4f}\")\n",
    "print(f\"Testing Recall: {np.mean(test_recalls):.4f} ± {np.std(test_recalls):.4f}\")\n",
    "print(f\"Testing F1-Score: {np.mean(test_f1s):.4f} ± {np.std(test_f1s):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
