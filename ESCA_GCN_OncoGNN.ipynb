{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e05039-c92b-479d-a5cd-da7ee6a298a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(314159) # set random seed\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd92cf2-e575-46de-bb75-e5ebd239ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load the edge list\n",
    "edge_list = np.load(\"edge_list_latest1.npy\", allow_pickle=True)\n",
    "\n",
    "edge_list = np.array([row[0].split(\",\") for row in edge_list[1:]])\n",
    "\n",
    "# Extract the source nodes, target nodes, and combined scores\n",
    "protein1 = edge_list[:, 0] \n",
    "protein2 = edge_list[:, 1] \n",
    "combined_score = edge_list[:, 2].astype(np.float32)\n",
    "\n",
    "# Mapping\n",
    "protein_map = {protein: idx for idx, protein in enumerate(np.unique(np.concatenate((protein1, protein2))))}\n",
    "protein1 = np.vectorize(protein_map.get)(protein1)\n",
    "protein2 = np.vectorize(protein_map.get)(protein2)\n",
    "\n",
    "# Stacking\n",
    "edges = np.stack([protein1, protein2], axis=0)\n",
    "edge_list = torch.tensor(edges, dtype=torch.long)  # Edge index tensor\n",
    "edge_weight = torch.tensor(combined_score, dtype=torch.float32)  # Edge weight tensor\n",
    "\n",
    "print(\"Edge list shape:\", edge_list.shape)  # Should be [2, n_edges]\n",
    "print(\"Edge weights shape:\", edge_weight.shape)  # Should be [n_edges]\n",
    "print(\"Protein map:\", protein_map)  # Mapping of protein names to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7181ccde-dc14-40d3-b106-482c08bafc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '200_node_network_embeddings_latest.csv'# Network embeddings\n",
    "node_dataset = pd.read_csv(data_path, index_col=0)\n",
    "node_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1000d69a-90f2-4005-ae26-e60044445b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "xcell_path = \"/home/user/Diamond_101225/diamond_rho/diamond_tpm/gene_xcell_spearman_rho_with_ENSG.csv\"\n",
    "print(\"Loading xCell PCs from:\", xcell_path)\n",
    "xcell = pd.read_csv(xcell_path, sep=None, engine='python')\n",
    "if 'gene_id' in xcell.columns:\n",
    "    gene_col = 'gene_id'\n",
    "    xcell['ensembl'] = xcell['gene_id'].astype(str).str.split('.').str[0]\n",
    "else:\n",
    "    gene_col = xcell.columns[0]\n",
    "    xcell['ensembl'] = xcell[gene_col].astype(str).str.split('.').str[0]\n",
    "\n",
    "pc_cols = [c for c in xcell.columns if ('xcell' in str(c).lower() and 'pc' in str(c).lower()) or str(c).lower().startswith('xcell_pc')]\n",
    "\n",
    "if len(pc_cols) < 1:\n",
    "    possible = [c for c in xcell.columns if c not in (gene_col, 'Gene', 'ensembl')]\n",
    "    pc_cols = possible[:20]\n",
    "\n",
    "print(\"xCell PC columns (count={}): {}\".format(len(pc_cols), pc_cols[:30]))\n",
    "\n",
    "if len(pc_cols) == 0:\n",
    "    print(\"no xCell PC columns detected.\")\n",
    "\n",
    "else:\n",
    "    xcell_idx = xcell.set_index('ensembl')[pc_cols]\n",
    "\n",
    "    # Alignment\n",
    "    xcell_aligned = xcell_idx.reindex(node_dataset.index)\n",
    "\n",
    "    missing_pct = xcell_aligned.isna().mean(axis=0).round(3) * 100\n",
    "    print(\"Percent missing per xCell PC column:\\n\", missing_pct)\n",
    "\n",
    "    fill_strategy = \"mean\" \n",
    "    if fill_strategy == \"zero\":\n",
    "        xcell_aligned_filled = xcell_aligned.fillna(0.0)\n",
    "    else:\n",
    "        xcell_aligned_filled = xcell_aligned.fillna(xcell_aligned.mean())\n",
    "\n",
    "    xcell_aligned_filled.columns = [f\"xcell_{str(c)}\" if not str(c).startswith(\"xcell_\") else str(c) for c in xcell_aligned_filled.columns]\n",
    "\n",
    "    # concatenation\n",
    "    node_dataset = pd.concat([node_dataset, xcell_aligned_filled], axis=1)\n",
    "\n",
    "    # saving\n",
    "    out_merge = \"node_dataset_with_xcell_rho.csv\"\n",
    "    node_dataset.to_csv(out_merge)\n",
    "    print(\"Saved merged node_dataset to:\", out_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9e5271-40f3-4c87-9e8c-297ad3ba6aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dataset.sort_index(inplace=True)\n",
    "node_dataset.reset_index(drop=False, inplace=True)\n",
    "assert((node_dataset.index.to_numpy()==np.arange(len(node_dataset))).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56534a3-8741-4fa4-9e0a-afbc6bf4ae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name = 'my_label'\n",
    "\n",
    "# find positives\n",
    "pos_label_col = 'gda_score' \n",
    "node_dataset[pos_label_col].fillna(0, inplace=True) \n",
    "pos_labels = pd.array([1 if row[pos_label_col] else None for id_, row in node_dataset.iterrows()], dtype='Int32')\n",
    "node_dataset[label_name] = pos_labels\n",
    "\n",
    "def sample_negatives(PU_labels):\n",
    "    '''randomly samples from the unlabeled samples'''\n",
    "    num_pos = (PU_labels==1).sum()\n",
    "    neg_inds = PU_labels[PU_labels.isna()].sample(num_pos).index\n",
    "\n",
    "    return neg_inds\n",
    "\n",
    "neg_label_inds = sample_negatives(node_dataset[label_name])\n",
    "node_dataset.loc[neg_label_inds, label_name] = 0\n",
    "\n",
    "node_dataset[label_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01855daf-394f-406d-9096-60919e5983b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dataset.set_index(node_dataset.columns[0], inplace=True)\n",
    "\n",
    "node_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bf5c72-3680-4784-b953-6608c7c6a847",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = node_dataset.columns[-1] \n",
    "node_dataset[label_col] = node_dataset[label_col].astype('Int32')\n",
    "\n",
    "# Remove the 'gda_score' column\n",
    "if 'gda_score' in node_dataset.columns:\n",
    "    node_dataset = node_dataset.drop(columns=['gda_score'])\n",
    "\n",
    "node_feat_cols = node_dataset.columns[:-1].tolist()  # All columns except the label\n",
    "\n",
    "\n",
    "# Get subset of node features + labels\n",
    "node_data = node_dataset[node_feat_cols + [label_col]]\n",
    "\n",
    "# Convert features to PyTorch Tensor\n",
    "X = torch.Tensor(node_data[node_feat_cols].select_dtypes(include=[np.number]).to_numpy(dtype=np.float32))\n",
    "\n",
    "\n",
    "# Convert labels to PyTorch Tensor, filling NaN with -1\n",
    "y = node_data[label_col].fillna(-1).astype('int')\n",
    "y = torch.Tensor(y).type(torch.int64)\n",
    "\n",
    "# Restrict to data with labels (non-NaN)\n",
    "node_data_labeled = node_data[node_data[label_col].notna()]\n",
    "node_data_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857c2fe7-a663-416e-88bc-0501c47235bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Extract the labeled node indices and labels\n",
    "X_myIDs = node_data_labeled.index.to_numpy() \n",
    "labels = node_data_labeled[label_col].to_numpy() \n",
    "\n",
    "test_size = 0.2\n",
    "val_size = 0.1 * (1 / (1 - test_size)) \n",
    "\n",
    "# Perform stratified train-test split\n",
    "myIDs_train_val, myIDs_test = train_test_split(X_myIDs, test_size=test_size, shuffle=True, stratify=labels)\n",
    "\n",
    "# Perform stratified train-validation split\n",
    "labels_train_val = node_data_labeled.loc[myIDs_train_val, label_col].to_numpy()\n",
    "myIDs_train, myIDs_val = train_test_split(myIDs_train_val, test_size=val_size, shuffle=True, stratify=labels_train_val)\n",
    "\n",
    "# Convert Ensembl IDs to Integer-Based Indices\n",
    "# Create a mapping from Ensembl ID to row position\n",
    "id_to_idx = {id_: idx for idx, id_ in enumerate(node_data.index)}\n",
    "\n",
    "# Map train, val, and test IDs to their corresponding row indices\n",
    "train_idx = np.array([id_to_idx[i] for i in myIDs_train])\n",
    "val_idx = np.array([id_to_idx[i] for i in myIDs_val])\n",
    "test_idx = np.array([id_to_idx[i] for i in myIDs_test])\n",
    "\n",
    "# Create boolean masks\n",
    "n_nodes = len(node_data)\n",
    "\n",
    "train_mask = np.zeros(n_nodes, dtype=bool)\n",
    "train_mask[train_idx] = True\n",
    "train_mask = torch.tensor(train_mask, dtype=torch.bool)\n",
    "\n",
    "val_mask = np.zeros(n_nodes, dtype=bool)\n",
    "val_mask[val_idx] = True\n",
    "val_mask = torch.tensor(val_mask, dtype=torch.bool)\n",
    "\n",
    "test_mask = np.zeros(n_nodes, dtype=bool)\n",
    "test_mask[test_idx] = True\n",
    "test_mask = torch.tensor(test_mask, dtype=torch.bool)\n",
    "\n",
    "print(f\"Number of training nodes: {train_mask.sum().item()}\")\n",
    "print(f\"Number of validation nodes: {val_mask.sum().item()}\")\n",
    "print(f\"Number of test nodes: {test_mask.sum().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4680d2a9-a288-4f8f-a147-cd08e720f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(x=X, y=y, edge_index=edge_list, edge_attr=edge_weight)  # Add edge weights\n",
    "\n",
    "num_classes = 2\n",
    "num_features = X.shape[1]\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask\n",
    "data.test_mask = test_mask\n",
    "assert y.shape[0] == X.shape[0], \"Mismatch: y and X must have the same number of nodes\"\n",
    "\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f97aa9-2765-43ce-98f5-55ee65bbb8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"\" #WANDB API KEY\n",
    "os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "\n",
    "config = {\n",
    "    \"dataset\": \"CIFAR10\",\n",
    "    \"machine\": \"online cluster\",\n",
    "    \"model\": \"CNN\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "\n",
    "wandb.init(project=\"offline-demo\")\n",
    "\n",
    "for i in range(100):\n",
    "    wandb.log({\"accuracy\": i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33301662-48a8-48a2-9d60-1d2371afd870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "# GNN architecture\n",
    "class GNNModel(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes, hidden_dense, GNN_conv_layer=GCNConv, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GNN_conv_layer(in_channels=num_features, out_channels=hidden_channels[0], **kwargs))\n",
    "        for c1, c2 in zip(hidden_channels[:-1], hidden_channels[1:]):\n",
    "            self.convs.append(GNN_conv_layer(in_channels=c1, out_channels=c2, **kwargs))\n",
    "        self.dense1 = torch.nn.Linear(hidden_channels[-1], hidden_dense)\n",
    "        self.dense_out = torch.nn.Linear(hidden_dense, num_classes)\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, edge_weight=edge_weight)\n",
    "            x = x.relu()\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = self.dense_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class LitGNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_name, num_features, hidden_channels, num_classes, hidden_dense, GNN_conv_layer, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.model = GNNModel(num_features, hidden_channels, num_classes, hidden_dense, GNN_conv_layer, dropout_rate)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        return self.model(x, edge_index, edge_weight)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        out = self(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        valid_indices = batch.y >= 0\n",
    "\n",
    "        if valid_indices.any():\n",
    "            loss = self.criterion(out[valid_indices], batch.y[valid_indices])\n",
    "        else:\n",
    "            loss = torch.tensor(0.0, requires_grad=True, device=self.device)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        out = self(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        valid_indices = batch.y >= 0\n",
    "\n",
    "        if valid_indices.any():\n",
    "            loss = self.criterion(out[valid_indices], batch.y[valid_indices])\n",
    "            acc = (out[valid_indices].argmax(dim=1) == batch.y[valid_indices]).float().mean()\n",
    "        else:\n",
    "            loss = torch.tensor(0.0, device=self.device)\n",
    "            acc = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        self.validation_step_outputs.append({\"val_loss\": loss, \"val_acc\": acc})\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        val_acc_mean = torch.stack([x[\"val_acc\"] for x in self.validation_step_outputs]).mean()\n",
    "        self.log(\"val_acc_epoch\", val_acc_mean, prog_bar=True)\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        out = self(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        valid_indices = batch.y >= 0\n",
    "\n",
    "        if valid_indices.any():\n",
    "            loss = self.criterion(out[valid_indices], batch.y[valid_indices])\n",
    "            acc = (out[valid_indices].argmax(dim=1) == batch.y[valid_indices]).float().mean()\n",
    "        else:\n",
    "            loss = torch.tensor(0.0, device=self.device)\n",
    "            acc = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, on_epoch=True, prog_bar=True)\n",
    "        return {\"test_loss\": loss, \"test_acc\": acc}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        test_acc_mean = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\n",
    "        self.log(\"test_acc_epoch\", test_acc_mean, prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c329632-7829-46f1-998c-59b26b0b926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, ModelSummary, EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import datetime\n",
    "\n",
    "# Set device to CPU\n",
    "AVAIL_GPUS = 0  # Use 0 to force CPU usage\n",
    "MAX_EPOCHS = 200  # Set maximum epochs\n",
    "NUM_TRIALS = 25   # Number of trials to run\n",
    "\n",
    "# Loop through trials\n",
    "for trial in range(1, NUM_TRIALS + 1):\n",
    "    # Set up model name and logger for each trial\n",
    "    model_name = f'gat_{datetime.datetime.today().strftime(\"%Y-%m-%d\")}_trial_{trial}'\n",
    "    logger = WandbLogger(name=model_name, project=\"Project X\", log_model=\"all\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LitGNN(\n",
    "        model_name=model_name, \n",
    "        num_features=num_features, \n",
    "        hidden_channels=[128], \n",
    "        num_classes=num_classes, \n",
    "        hidden_dense=64, \n",
    "        GNN_conv_layer=GCNConv, \n",
    "        dropout_rate=0.1\n",
    "    )\n",
    "\n",
    "    # Create DataLoaders for training and validation\n",
    "    train_data_loader = DataLoader([data], batch_size=1, num_workers=3)  # Training DataLoader\n",
    "    val_data_loader = DataLoader([data], batch_size=1, num_workers=3)    # Validation DataLoader\n",
    "\n",
    "    # Set up ModelCheckpoint\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        save_weights_only=True,  # Save only the weights\n",
    "        mode=\"max\", \n",
    "        monitor=\"val_acc\",  # Monitor validation accuracy\n",
    "        dirpath=f\"checkpoints/trial_{trial}\",  # Separate directory for each trial\n",
    "        filename=\"{epoch:02d}-{val_acc:.2f}\"  # Filename format\n",
    "    )\n",
    "\n",
    "    # Set up Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        callbacks=[\n",
    "            checkpoint_callback, \n",
    "            EarlyStopping(monitor=\"val_acc\", patience=50, verbose=True, mode=\"max\"),  # Monitor validation accuracy\n",
    "            ModelSummary(max_depth=3)  # Summary of the model\n",
    "        ],\n",
    "        devices=1,  # Use specified number of devices\n",
    "        accelerator=\"cpu\",  # Explicitly specify CPU\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, train_dataloaders=train_data_loader, val_dataloaders=val_data_loader)\n",
    "\n",
    "    # Load the best checkpoint\n",
    "    best_model_path = checkpoint_callback.best_model_path\n",
    "    if best_model_path:\n",
    "        model = LitGNN.load_from_checkpoint(\n",
    "            best_model_path,\n",
    "            model_name=model_name,\n",
    "            num_features=num_features,\n",
    "            hidden_channels=[128],\n",
    "            num_classes=num_classes,\n",
    "            hidden_dense=64,\n",
    "            GNN_conv_layer=GCNConv,\n",
    "            dropout_rate=0.1\n",
    "        )\n",
    "        print(f\"Trial {trial}: Loaded model from checkpoint: {best_model_path}\")\n",
    "    else:\n",
    "        print(f\"Trial {trial}: No checkpoint found. Training from scratch.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
